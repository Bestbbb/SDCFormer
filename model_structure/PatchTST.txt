Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='weather_96_96', model='PatchTST', data='custom', root_path='./dataset/weather/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, seasonal_patterns='Monthly', mask_rate=0.25, anomaly_ratio=0.25, top_k=5, num_kernels=6, enc_in=21, dec_in=21, c_out=21, d_model=512, n_heads=4, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=3, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2, head_dropout=0.0, patch_len=16, stride=8, show_model=True)
Use GPU: cuda:0
model Model(
  (patch_embedding): PatchEmbedding(
    (padding_patch_layer): ReplicationPad1d((0, 8))
    (value_embedding): TokenEmbedding(
      (tokenConv): Conv1d(16, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)
    )
    (position_embedding): PositionalEmbedding()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0): EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (head): FlattenHead(
    (flatten): Flatten(start_dim=-2, end_dim=-1)
    (linear): Linear(in_features=6144, out_features=96, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
Model                                         [32, 96, 21]              --
├─PatchEmbedding: 1-1                         [672, 12, 512]            --
│    └─ReplicationPad1d: 2-1                  [32, 21, 104]             --
│    └─TokenEmbedding: 2-2                    [672, 12, 512]            --
│    │    └─Conv1d: 3-1                       [672, 512, 12]            24,576
│    └─PositionalEmbedding: 2-3               [1, 12, 512]              --
│    └─Dropout: 2-4                           [672, 12, 512]            --
├─Encoder: 1-2                                [672, 12, 512]            --
│    └─ModuleList: 2-5                        --                        --
│    │    └─EncoderLayer: 3-2                 [672, 12, 512]            3,152,384
│    │    └─EncoderLayer: 3-3                 [672, 12, 512]            3,152,384
│    └─LayerNorm: 2-6                         [672, 12, 512]            1,024
├─FlattenHead: 1-3                            [32, 21, 96]              --
│    └─Flatten: 2-7                           [32, 21, 6144]            --
│    └─Linear: 2-8                            [32, 21, 96]              589,920
│    └─Dropout: 2-9                           [32, 21, 96]              --
===============================================================================================
Total params: 6,920,288
Trainable params: 6,920,288
Non-trainable params: 0
Total mult-adds (G): 35.50
===============================================================================================
Input size (MB): 0.77
Forward/backward pass size (MB): 793.24
Params size (MB): 27.68
Estimated Total Size (MB): 821.69
===============================================================================================
