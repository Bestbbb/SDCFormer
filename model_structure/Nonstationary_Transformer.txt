Args in experiment:
Namespace(task_name='long_term_forecast', is_training=1, model_id='weather_96_96', model='Nonstationary_Transformer', data='custom', root_path='./dataset/weather/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, seasonal_patterns='Monthly', mask_rate=0.25, anomaly_ratio=0.25, top_k=5, num_kernels=6, enc_in=21, dec_in=21, c_out=21, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=1, train_epochs=3, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[256, 256], p_hidden_layers=2, head_dropout=0.0, patch_len=16, stride=8, show_model=True)
Use GPU: cuda:0
model Model(
  (enc_embedding): DataEmbedding(
    (value_embedding): TokenEmbedding(
      (tokenConv): Conv1d(21, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)
    )
    (position_embedding): PositionalEmbedding()
    (temporal_embedding): TimeFeatureEmbedding(
      (embed): Linear(in_features=4, out_features=512, bias=False)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0): EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): DSAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): DSAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (dec_embedding): DataEmbedding(
    (value_embedding): TokenEmbedding(
      (tokenConv): Conv1d(21, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)
    )
    (position_embedding): PositionalEmbedding()
    (temporal_embedding): TimeFeatureEmbedding(
      (embed): Linear(in_features=4, out_features=512, bias=False)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attention): AttentionLayer(
          (inner_attention): DSAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (cross_attention): AttentionLayer(
          (inner_attention): DSAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (projection): Linear(in_features=512, out_features=21, bias=True)
  )
  (tau_learner): Projector(
    (series_conv): Conv1d(96, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)
    (backbone): Sequential(
      (0): Linear(in_features=42, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ReLU()
      (4): Linear(in_features=256, out_features=1, bias=False)
    )
  )
  (delta_learner): Projector(
    (series_conv): Conv1d(96, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)
    (backbone): Sequential(
      (0): Linear(in_features=42, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ReLU()
      (4): Linear(in_features=256, out_features=96, bias=False)
    )
  )
)
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
Model                                         [32, 96, 21]              --
├─Projector: 1-1                              [32, 1]                   --
│    └─Conv1d: 2-1                            [32, 1, 21]               288
│    └─Sequential: 2-2                        [32, 1]                   --
│    │    └─Linear: 3-1                       [32, 256]                 11,008
│    │    └─ReLU: 3-2                         [32, 256]                 --
│    │    └─Linear: 3-3                       [32, 256]                 65,792
│    │    └─ReLU: 3-4                         [32, 256]                 --
│    │    └─Linear: 3-5                       [32, 1]                   256
├─Projector: 1-2                              [32, 96]                  --
│    └─Conv1d: 2-3                            [32, 1, 21]               288
│    └─Sequential: 2-4                        [32, 96]                  --
│    │    └─Linear: 3-6                       [32, 256]                 11,008
│    │    └─ReLU: 3-7                         [32, 256]                 --
│    │    └─Linear: 3-8                       [32, 256]                 65,792
│    │    └─ReLU: 3-9                         [32, 256]                 --
│    │    └─Linear: 3-10                      [32, 96]                  24,576
├─DataEmbedding: 1-3                          [32, 96, 512]             --
│    └─TokenEmbedding: 2-5                    [32, 96, 512]             --
│    │    └─Conv1d: 3-11                      [32, 512, 96]             32,256
│    └─TimeFeatureEmbedding: 2-6              [32, 96, 512]             --
│    │    └─Linear: 3-12                      [32, 96, 512]             2,048
│    └─PositionalEmbedding: 2-7               [1, 96, 512]              --
│    └─Dropout: 2-8                           [32, 96, 512]             --
├─Encoder: 1-4                                [32, 96, 512]             --
│    └─ModuleList: 2-9                        --                        --
│    │    └─EncoderLayer: 3-13                [32, 96, 512]             3,152,384
│    │    └─EncoderLayer: 3-14                [32, 96, 512]             3,152,384
│    └─LayerNorm: 2-10                        [32, 96, 512]             1,024
├─DataEmbedding: 1-5                          [32, 144, 512]            --
│    └─TokenEmbedding: 2-11                   [32, 144, 512]            --
│    │    └─Conv1d: 3-15                      [32, 512, 144]            32,256
│    └─TimeFeatureEmbedding: 2-12             [32, 144, 512]            --
│    │    └─Linear: 3-16                      [32, 144, 512]            2,048
│    └─PositionalEmbedding: 2-13              [1, 144, 512]             --
│    └─Dropout: 2-14                          [32, 144, 512]            --
├─Decoder: 1-6                                [32, 144, 21]             --
│    └─ModuleList: 2-15                       --                        --
│    │    └─DecoderLayer: 3-17                [32, 144, 512]            4,204,032
│    └─LayerNorm: 2-16                        [32, 144, 512]            1,024
│    └─Linear: 2-17                           [32, 144, 21]             10,773
===============================================================================================
Total params: 10,769,237
Trainable params: 10,769,237
Non-trainable params: 0
Total mult-adds (G): 22.97
===============================================================================================
Input size (MB): 0.77
Forward/backward pass size (MB): 661.67
Params size (MB): 43.08
Estimated Total Size (MB): 705.52
===============================================================================================
